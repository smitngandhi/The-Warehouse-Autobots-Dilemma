{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agents...\n",
      "Visualizing the grid and paths...\n",
      "Average is: 7.833333333333333\n",
      "Total time taken:  66.229820728302 seconds\n",
      "Bot 0 individual steps this round: 23\n",
      "Bot 1 individual steps this round: 2\n",
      "Bot 2 individual steps this round: 2\n",
      "Bot 3 individual steps this round: 8\n",
      "Bot 4 individual steps this round: 9\n",
      "Bot 5 individual steps this round: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tkinter as tk\n",
    "import time\n",
    "import heapq\n",
    "import tkinter.simpledialog as simpledialog\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, position, parent=None):\n",
    "        self.position = position\n",
    "        self.parent = parent\n",
    "        self.g = 0\n",
    "        self.h = 0\n",
    "        self.f = 0\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.f < other.f\n",
    "\n",
    "def heuristic(a, b):\n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "def a_star_search(grid, start, goal):\n",
    "    open_list = []\n",
    "    closed_set = set()\n",
    "\n",
    "    start_node = Node(start)\n",
    "    goal_node = Node(goal)\n",
    "\n",
    "    heapq.heappush(open_list, start_node)\n",
    "\n",
    "    while open_list:\n",
    "        current_node = heapq.heappop(open_list)\n",
    "\n",
    "        if current_node.position == goal:\n",
    "            path = []\n",
    "            while current_node:\n",
    "                path.append(current_node.position)\n",
    "                current_node = current_node.parent\n",
    "            return path[::-1]\n",
    "\n",
    "        closed_set.add(current_node.position)\n",
    "\n",
    "        for new_position in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n",
    "            node_position = (current_node.position[0] + new_position[0], current_node.position[1] + new_position[1])\n",
    "\n",
    "            if (0 <= node_position[0] < len(grid)) and (0 <= node_position[1] < len(grid[0])):\n",
    "                if grid[node_position[0]][node_position[1]] == 1 and node_position not in closed_set:\n",
    "                    child_node = Node(node_position, current_node)\n",
    "                    child_node.g = current_node.g + 1\n",
    "                    child_node.h = heuristic(child_node.position, goal_node.position)\n",
    "                    child_node.f = child_node.g + child_node.h\n",
    "\n",
    "                    if add_to_open(open_list, child_node):\n",
    "                        heapq.heappush(open_list, child_node)\n",
    "\n",
    "    return None\n",
    "\n",
    "def add_to_open(open_list, child_node):\n",
    "    for node in open_list:\n",
    "        if child_node.position == node.position and child_node.g > node.g:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, agent_id, grid_size, obstacles, start, goal, initial_path, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.agent_id = agent_id\n",
    "        self.grid_size = grid_size\n",
    "        self.obstacles = obstacles\n",
    "        self.start = start\n",
    "        self.initial_start = start  # Store the initial start position\n",
    "        self.goal = goal\n",
    "        self.current_position = start\n",
    "        self.initial_path = initial_path\n",
    "        self.q_table = np.zeros((*grid_size, 5))  # 4 directions + 1 for staying in place\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0), (0, 0)]  # Right, Down, Left, Up, Stay\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(range(5))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state[0], state[1]])\n",
    "\n",
    "    def take_action(self, action):\n",
    "        next_position = (self.current_position[0] + self.actions[action][0],\n",
    "                         self.current_position[1] + self.actions[action][1])\n",
    "        if (0 <= next_position[0] < self.grid_size[0] and\n",
    "            0 <= next_position[1] < self.grid_size[1] and\n",
    "            next_position not in self.obstacles):\n",
    "            return next_position\n",
    "        return self.current_position\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state[0], next_state[1]])\n",
    "        td_target = reward + self.gamma * self.q_table[next_state[0], next_state[1], best_next_action]\n",
    "        td_error = td_target - self.q_table[state[0], state[1], action]\n",
    "        self.q_table[state[0], state[1], action] += self.alpha * td_error\n",
    "\n",
    "class MultiAgentEnvironment:\n",
    "    def __init__(self, grid, agents):\n",
    "        self.grid = grid\n",
    "        self.agents = agents\n",
    "        self.initial_positions = [agent.start for agent in agents]  # Store initial positions\n",
    "\n",
    "    def reset(self):\n",
    "        for agent, initial_pos in zip(self.agents, self.initial_positions):\n",
    "            agent.current_position = initial_pos\n",
    "\n",
    "    def step(self):\n",
    "        new_positions = {}\n",
    "        collisions = set()\n",
    "        actions_taken = {}\n",
    "\n",
    "        # First pass: Check next positions for potential collisions\n",
    "        for agent in self.agents:\n",
    "            action = agent.get_next_action(agent.current_position)\n",
    "            next_position = agent.take_action(action)\n",
    "\n",
    "            if next_position in new_positions:\n",
    "                collisions.add(next_position)\n",
    "            else:\n",
    "                new_positions[next_position] = agent\n",
    "                actions_taken[agent.agent_id] = next_position\n",
    "\n",
    "        # Second pass: Move agents, handle collisions\n",
    "        for agent in self.agents:\n",
    "            if agent.agent_id in actions_taken:\n",
    "                next_position = actions_taken[agent.agent_id]\n",
    "\n",
    "                if next_position in collisions:\n",
    "                    # Collision detected, stop this agent from moving\n",
    "                    reward = -25  # Collision penalty\n",
    "                    next_position = agent.current_position  # Stay in place\n",
    "                elif next_position == agent.goal:\n",
    "                    reward = 10  # Goal reward\n",
    "                else:\n",
    "                    reward = -1  # Step penalty\n",
    "                \n",
    "                agent.update_q_table(agent.current_position, agent.get_next_action(agent.current_position), reward, next_position)\n",
    "                agent.current_position = next_position\n",
    "            else:\n",
    "                # This agent did not get to move (it is colliding)\n",
    "                agent.update_q_table(agent.current_position, agent.get_next_action(agent.current_position), -5, agent.current_position)  # Staying in place penalty\n",
    "\n",
    "        return [agent.current_position for agent in self.agents]\n",
    "\n",
    "# The rest of your BotGrid and other classes can remain the same.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BotGrid(tk.Tk):\n",
    "    def __init__(self, grid, agents, env):\n",
    "        super().__init__()\n",
    "        self.title(\"Multi-Agent Q-Learning with A* Path Planning\")\n",
    "\n",
    "        self.grid = grid\n",
    "        self.agents = agents\n",
    "        self.env = env\n",
    "\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.cell_size = 100\n",
    "\n",
    "        self.canvas = tk.Canvas(self, width=self.cols * self.cell_size, height=self.rows * self.cell_size)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        self.create_grid()\n",
    "        self.bot_squares = self.create_bots()\n",
    "        self.individual_steps = [0] * len(self.agents)\n",
    "        self.start_time = time.time()\n",
    "        self.steps = [0 for _ in agents]\n",
    "        self.steps_array = [0] * len(agents) \n",
    "        self.timer_label = tk.Label(self, text=\"Time: 0s\")\n",
    "        self.timer_label.pack()\n",
    "        # self.steps_label = tk.Label(self, text=\", \".join([f\"Bot {i} steps: 0\" for i in range(len(agents))]))\n",
    "        # self.steps_label.pack()\n",
    "\n",
    "        # self.avg_steps_label = tk.Label(self, text=\"Average Steps: 0.00\")\n",
    "        # self.avg_steps_label.pack()\n",
    "        # Reset agents to their initial positions before starting visualization\n",
    "        self.env.reset()\n",
    "\n",
    "        self.move_bots()\n",
    "\n",
    "    def calculate_average_steps(self):\n",
    "        if len(self.steps) == 0:  # Avoid division by zero\n",
    "            avg_steps = 0\n",
    "        else:\n",
    "            avg_steps = sum(self.steps) / len(self.steps)\n",
    "            print(f\"Average is: {avg_steps}\")\n",
    "        \n",
    "\n",
    "    def create_grid(self):\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                x1, y1 = c * self.cell_size, r * self.cell_size\n",
    "                x2, y2 = x1 + self.cell_size, y1 + self.cell_size\n",
    "                color = \"white\" if self.grid[r][c] == 1 else \"black\"\n",
    "                self.canvas.create_rectangle(x1, y1, x2, y2, fill=color)\n",
    "\n",
    "    def create_bots(self):\n",
    "        bot_squares = []\n",
    "        colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            r, c = agent.current_position\n",
    "            x1, y1 = c * self.cell_size, r * self.cell_size\n",
    "            x2, y2 = x1 + self.cell_size, y1 + self.cell_size\n",
    "            color = colors[i % len(colors)]\n",
    "            bot_square = self.canvas.create_rectangle(x1, y1, x2, y2, fill=color)\n",
    "            bot_squares.append(bot_square)\n",
    "        return bot_squares\n",
    "    def move_bots(self):\n",
    "        def move_step():\n",
    "            new_positions = self.env.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Create a dynamic list to hold individual step counts\n",
    "            \n",
    "              # Initialize with zero for each bot\n",
    "\n",
    "            for i, new_pos in enumerate(new_positions):\n",
    "\n",
    "                if self.agents[i].current_position == self.agents[i].goal:\n",
    "                    # print(f\"Bot {i} has reached the goal at {self.agents[i].goal}.\")\n",
    "                    if(self.steps[i] == 0):\n",
    "                        self.steps[i] = self.individual_steps[i] + 1\n",
    "                # Check if the bot has moved\n",
    "                if new_pos != self.agents[i].current_position:\n",
    "                    self.move_bot(i, new_pos)\n",
    "                    # self.steps[i] += 1  # Increment total steps for a move\n",
    "                    self.individual_steps[i] += 1  # Update individual steps\n",
    "                    # print(f\"Bot {i} moves to {new_pos}\")\n",
    "                else:  # If bot doesn't move, keep its position the same\n",
    "                    self.move_bot(i, self.agents[i].current_position)\n",
    "                    # self.steps[i] += 1  # Increment total steps for staying in place\n",
    "                    self.individual_steps[i] += 1  # Update individual steps\n",
    "                    # print(f\"Bot {i} waits at {self.agents[i].current_position}\")\n",
    "\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            self.timer_label.config(text=f\"Time: {int(elapsed_time)}s\")\n",
    "        \n",
    "            # self.steps_label.config(text=\", \".join([f\"Bot {i} steps: {self.individual_steps[i]}\" for i in range(len(self.agents))]))\n",
    "\n",
    "            if not all(agent.current_position == agent.goal for agent in self.agents):\n",
    "                self.after(3000, move_step)\n",
    "            else:\n",
    "                self.calculate_average_steps()\n",
    "                # print(f\"Average Steps: {avg_steps:.2f}\")\n",
    "                # self.avg_steps_label.config(text=f\"Average Steps: {avg_steps:.2f}\")\n",
    "\n",
    "                # Print the steps for each bot, using the dynamic list\n",
    "                print(f'Total time taken:  {elapsed_time} seconds')\n",
    "                for i, steps in enumerate(self.steps):\n",
    "                    print(f\"Bot {i} individual steps this round: {steps}\")\n",
    "\n",
    "        move_step()\n",
    "    def move_bot(self, bot_index, new_position):\n",
    "        r, c = new_position\n",
    "        x1, y1 = c * self.cell_size, r * self.cell_size\n",
    "        x2, y2 = x1 + self.cell_size, y1 + self.cell_size\n",
    "        self.canvas.coords(self.bot_squares[bot_index], x1, y1, x2, y2)\n",
    "\n",
    "def run_hybrid_q_learning_demo():\n",
    "    # User input for grid size\n",
    "    rows = int(simpledialog.askstring(\"Grid Size\", \"Enter number of rows:\"))\n",
    "    cols = int(simpledialog.askstring(\"Grid Size\", \"Enter number of columns:\"))\n",
    "\n",
    "    # Initialize the grid\n",
    "    grid = [[1 for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "    # Get obstacles from the user\n",
    "    obstacle_input = simpledialog.askstring(\"Obstacles Input\", \"Enter obstacles in the form 'row,col' (e.g., '1,1;2,3'):\").strip()\n",
    "    obstacles = set()\n",
    "    if obstacle_input:\n",
    "        for obs in obstacle_input.split(';'):\n",
    "            r, c = map(int, obs.split(','))\n",
    "            grid[r][c] = 0\n",
    "            obstacles.add((r, c))\n",
    "\n",
    "    # Get bot configurations from the user\n",
    "    num_bots = int(simpledialog.askstring(\"Bots Input\", \"Enter number of bots:\"))\n",
    "    agents = []\n",
    "    for i in range(num_bots):\n",
    "        start = tuple(map(int, simpledialog.askstring(\"Bot Input\", f\"Enter start position for Bot {i + 1} (row,col):\").split(',')))\n",
    "        goal = tuple(map(int, simpledialog.askstring(\"Bot Input\", f\"Enter goal position for Bot {i + 1} (row,col):\").split(',')))\n",
    "        \n",
    "        # Generate initial path using A*\n",
    "        initial_path = a_star_search(grid, start, goal)\n",
    "        \n",
    "        agent = QLearningAgent(i, (rows, cols), obstacles, start, goal, initial_path)\n",
    "        agents.append(agent)\n",
    "\n",
    "    env = MultiAgentEnvironment(grid, agents)\n",
    "\n",
    "    # Train agents\n",
    "    print(\"Training agents...\")\n",
    "    for episode in range(6000):\n",
    "        env.reset()  # Reset to initial positions at the start of each episode\n",
    "        while not all(agent.current_position == agent.goal for agent in agents):\n",
    "            env.step()\n",
    "\n",
    "    # Reset agents to initial positions for visualization\n",
    "    env.reset()\n",
    "    print(\"Visualizing the grid and paths...\")\n",
    "    app = BotGrid(grid, agents, env)\n",
    "    app.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_hybrid_q_learning_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
