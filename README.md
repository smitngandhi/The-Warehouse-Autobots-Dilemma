# The_Outliers

**Hackathon Project: DoseHack**

This repository showcases our solution for efficient and intelligent pathfinding for single and multiple autonomous bots using both classical and reinforcement learning methods.

---

## ğŸ“Œ Project Modules

### ğŸš¶â€â™‚ï¸ Movement of Single Bot â€“ `Movement_single.ipynb`

This module implements the **A\*** algorithm to determine the shortest path for a single bot moving from a given source to a destination on a grid.

- The bot starts facing **upward**.
- Movement instructions include:
  - **Forward**: Move in the current direction.
  - **Left**: Rotate 90Â° counterclockwise.
  - **Right**: Rotate 90Â° clockwise.
- When a direction change is required, the bot first rotates appropriately before moving forward.
- The A\* algorithm calculates the optimal path while avoiding obstacles, ensuring an efficient route to the destination.

---

### ğŸ¤– Coordinated Movement of Dual Bots â€“ `dual_bots.ipynb`

This notebook implements the **A\*** algorithm to coordinate movement between **two bots** from their respective sources to destinations.

- Collision avoidance is handled by allowing one bot to **wait** if a collision is predicted, ensuring safe and efficient parallel movement.

---

### ğŸ–¼ï¸ GUI Display of Dual Bots â€“ `gui_dual.ipynb`

A Tkinter-based GUI visualization of the dual bot A\* algorithm.

- Offers a visual interface for tracking the real-time pathfinding and movements of the bots on a grid.

---

### ğŸ§  Q-Learning for Multi-Bots â€“ `q_learning_bot.ipynb`

This module applies **Q-Learning** for pathfinding involving a **dynamic number of bots**:

- Users can input the number of bots dynamically.
- Each bot finds its own path toward its destination using reinforcement learning.
- The process stops as soon as the first successful path is found, saving both **time** and **resources**.

---

### ğŸ Final Optimized Q-Learning Model â€“ `q_learning.ipynb`

This notebook contains our **final implementation** of the Q-Learning approach with optimized performance metrics.

- Output includes:
  - â±ï¸ **Time Taken**
  - ğŸ“Š **Average Steps**
  - ğŸ‘£ **Each Bot's Individual Steps**

---

## ğŸš€ Tech Stack

- Python
- A* Algorithm
- Q-Learning (Reinforcement Learning)
- Tkinter (for GUI)

---

## ğŸ’¡ Developed at

**DoseHack** Hackathon â€“ Team: *The_Outliers*

---

## ğŸ“‚ Repository Structure

```bash
â”œâ”€â”€ Movement_single.ipynb         # A* for single bot
â”œâ”€â”€ dual_bots.ipynb               # A* for dual bots with collision handling
â”œâ”€â”€ gui_dual.ipynb                # GUI visualization for dual bots
â”œâ”€â”€ q_learning_bot.ipynb          # Q-Learning for multi-bots (dynamic)
â”œâ”€â”€ q_learning.ipynb              # Final Q-Learning optimized solution
â”œâ”€â”€ README.md                     # Project documentation
```

---

## âœ¨ Contributors

- [Dhruvil Joshi](https://github.com/Dhruvil-Joshi)
- [Prachi Desai](https://github.com/Prachidesai2506)
---
